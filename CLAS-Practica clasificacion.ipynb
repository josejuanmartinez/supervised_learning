{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"name":"CLAS-Practica clasificacion.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"36tAu8vSzlTt"},"source":["\n","# Classifier comparison\n","\n","A comparison of a several classifiers in scikit-learn on synthetic datasets.\n","The point of this example is to illustrate the nature of decision boundaries\n","of different classifiers.\n","This should be taken with a grain of salt, as the intuition conveyed by\n","these examples does not necessarily carry over to real datasets.\n","\n","Particularly in high-dimensional spaces, data can more easily be separated\n","linearly and the simplicity of classifiers such as naive Bayes and linear SVMs\n","might lead to better generalization than is achieved by other classifiers.\n","\n","The plots show training points in solid colors and testing points\n","semi-transparent. The lower right shows the classification accuracy on the test\n","set.\n"]},{"cell_type":"markdown","metadata":{"id":"_JWu9rfX3Xdz"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"6_5wGLqF3WO_"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from matplotlib.colors import ListedColormap\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_moons, make_circles, make_classification\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import LinearSVC, SVC, NuSVC\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import *\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import plot_precision_recall_curve, precision_recall_fscore_support"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cFLFrxI73uLu"},"source":["## Shows by default the output of the plots to Google Colab"]},{"cell_type":"code","metadata":{"id":"T8LJAQNQzlTq"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f5mrSfQv32Te"},"source":["## Upload the CSV files from drawdata.xyz\n","You can draw and upload several! Feel free to experiment!"]},{"cell_type":"code","metadata":{"id":"yMYp19BM34S2"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XPJ-haPQ4NAD"},"source":["## Create a list in python with the name of all classifiers you have imported"]},{"cell_type":"code","metadata":{"id":"7iMBxAZs5XQN"},"source":["names = # <- create a list here "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sl8i_Kv4Z7V"},"source":["## TODO: Create a list in Python, with one instance of each classifier described above.\n","You may need to look at the imports to know how the module is called to instantiate it"]},{"cell_type":"markdown","metadata":{"id":"VqgS-Xno4qtT"},"source":["You can leave the default hyperparameters. However, to achieve better results, you may want to check documentation of all of them and see what parameters you can pass to the constructors:\n","\n","https://scikit-learn.org/stable/supervised_learning.html"]},{"cell_type":"code","metadata":{"id":"-MKMYaUgN76T"},"source":["classifiers = # <- create a list here "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iv3RTzo58dc"},"source":["## TODO: Read the csvs you have imported. Show the dataframe with the points from your drawing"]},{"cell_type":"code","metadata":{"id":"22Wre0pJ6BtV"},"source":["df = # <- read with pandas your dataframe "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OC8eGafF6ye1"},"source":["## Separate the predicted class in another dataframe"]},{"cell_type":"code","metadata":{"id":"5bcp6jdJAXy8"},"source":["df_points = df[['x', 'y']]\n","df_class = df[['z']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EimuOi8GAUeQ"},"source":["## TODO: Get the values of both dataframe. Use 'values' function"]},{"cell_type":"code","metadata":{"id":"0-zza2v26xPr"},"source":["df_points_values = # <- get values of df_points dataframe\n","df_class_values = # <- get values of df_class dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cV8JFTgBAmq1"},"source":["## Print df_train_values. It should be an array of [x,y] values"]},{"cell_type":"code","metadata":{"id":"zqGpTFXcAkmA"},"source":["df_points_values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJHzA32yAuzW"},"source":["## Print the class values. \n","You will see it's one array (list) per line. Scikit-learn wants 1 array at all, with all elements in a row. We concate them."]},{"cell_type":"code","metadata":{"id":"4-5KPywmAz4H"},"source":["df_class_values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Lo7x-eY_eJw"},"source":["df_class_values_concat = np.concatenate(df_class_values, axis=0)\n","df_class_values_concat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1nD7M_h57Lrl"},"source":["## TODO: Create a tuple with train and text "]},{"cell_type":"code","metadata":{"id":"amxWbNrK7PoA"},"source":["df_tuple = # <- create a tuple with 2 elements, df_points_values    and     df_class_values_concat\n","df_tuple"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-LqNlvzm9UWJ"},"source":["## Run this code that will:\n","1) For each dataset...\n","\n","2) ... for each classifier ...\n","\n","3) ......train.......\n","\n","4) ......predit......\n","\n","5) ......plot.\n"]},{"cell_type":"code","metadata":{"id":"khNoGMcvzlTu"},"source":["h=.2\n","figure = plt.figure(figsize=(27, 9))\n","i = 1\n","\n","# preprocess dataset, split into training and test part\n","X, y = df_tuple\n","X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=.4, random_state=42)\n","\n","x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n","y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                      np.arange(y_min, y_max, h))\n","\n","# just plot the dataset first\n","cm = plt.cm.RdBu\n","cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n","ax = plt.subplot(1, len(classifiers) + 1, i)\n","ax.set_title(\"Input data\")\n","# Plot the training points\n","#ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n","#           edgecolors='k')\n","ax.scatter(X_train[:, 0], X_train[:, 1], c='gold', cmap=cm_bright,\n","            edgecolors='k')\n","# Plot the testing points\n","ax.scatter(X_test[:, 0], X_test[:, 1], c='green', cmap=cm_bright, alpha=0.6,\n","            edgecolors='k')\n","ax.set_xlim(xx.min(), xx.max())\n","ax.set_ylim(yy.min(), yy.max())\n","ax.set_xticks(())\n","ax.set_yticks(())\n","i += 1\n","\n","# iterate over classifiers\n","for name, clf in zip(names, classifiers):\n","    ax = plt.subplot(1, len(classifiers) + 1, i)\n","    clf.fit(X_train, y_train)\n","    score = clf.score(X_test, y_test)\n","    \n","\n","    # Plot the decision boundary. For that, we will assign a color to each\n","    # point in the mesh [x_min, x_max]x[y_min, y_max].\n","    if hasattr(clf, \"decision_function\"):\n","        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","    else:\n","        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n","    \n","    # Put the result into a color plot\n","    try:\n","      Z = Z.reshape(xx.shape)\n","      ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n","    except:\n","      print('Error: ' + str(clf))\n","      pass\n","    \n","    # Plot the training points\n","    ax.scatter(X_train[:, 0], X_train[:, 1], c='gold', cmap=cm_bright,\n","                edgecolors='k')\n","    # Plot the testing points\n","    ax.scatter(X_test[:, 0], X_test[:, 1], c='green', cmap=cm_bright,\n","                edgecolors='k', alpha=0.6)\n","\n","    ax.set_xlim(xx.min(), xx.max())\n","    ax.set_ylim(yy.min(), yy.max())\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","    ax.set_title(name)\n","    ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n","            size=15, horizontalalignment='right')\n","    i += 1\n","\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_PBEZlzVXO38"},"source":["## Confusion Matrix\n","Run this code that will:\n","\n","1) For each classifier ...\n","\n","2) ......train.......\n","\n","3) ......predict......\n","\n","4) ......calculate confusion matrix...\n","\n","5) ......plot."]},{"cell_type":"code","metadata":{"id":"vdr7RuKGS4ce"},"source":["figure = plt.figure(figsize=(20, 2))\n","i = 1\n","\n","X, y = df_tuple\n","X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=.4, random_state=42)\n","counter = 0\n","# iterate over classifiers\n","for name, clf in zip(names, classifiers):\n","    ax = plt.subplot(1, len(classifiers) + 1, i)\n","    ax.set_title(names[counter])\n","    clf.fit(X_train, y_train)\n","    display = plot_confusion_matrix(clf, X_test, y_test, ax=ax)\n","    display.im_.colorbar.remove()\n","    i += 1\n","    counter += 1\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxOQjqU2kBXv"},"source":["## Roc Curve\n","Run this code that will:\n","\n","1) For each classifier ...\n","\n","2) ......train.......\n","\n","3) ......predict......\n","\n","4) ......calculate ROC and AUC...\n","\n","5) ......plot.\n"]},{"cell_type":"code","metadata":{"id":"S39uYCtakA6O"},"source":["X, y = df_tuple\n","X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=.4, random_state=42)\n","# iterate over classifiers\n","for name, clf in zip(names, classifiers):\n","    clf.fit(X_train, y_train)\n","    plot_roc_curve(clf, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MH3Y5EJVZiJb"},"source":["## Precision, Recall, F1\n","Run this code that will:\n","\n","1) For each classifier ...\n","\n","2) ......train.......\n","\n","3) ......predict......\n","\n","4) ......calculate metrics...\n","\n","5) ......print them."]},{"cell_type":"code","metadata":{"id":"6TFHUyKZU5HM"},"source":["# preprocess dataset, split into training and test part\n","X, y = df_tuple\n","X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=.4, random_state=42)\n","    \n","# iterate over classifiers\n","counter = 0\n","for name, clf in zip(names, classifiers):\n","    clf.fit(X_train, y_train)\n","    pred = clf.predict(X_test)\n","    precision, recall, f1, support = precision_recall_fscore_support(y_test, pred)\n","    print(f\"Classifier {names[counter]} metrics:\\n-P({precision})\\n-R({recall})\\n-F1({f1})\\n-Support=({support})\\n\")\n","    counter +=1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-Cfaq3qOuwp"},"source":["## Answer to the following questions with a colleague\n","\n","1.   Which one do you think is better for  your data?\n","2.   Do results differ much among models?\n","3.   What do the color gradients mean?\n","4.   How do the different models work? Discuss the theory.\n","5.   In Precision / Recall / F1, wy there are two values?\n","6.   In Precision / Recall / F1, what does 'Support' mean?\n","7.   What different metrics have we used? How do you interpret them?"]}]}